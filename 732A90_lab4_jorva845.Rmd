---
title: "732A90 Computational Statistics - Lab 4"
author: "Joris van Doorn - jorva845"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
RNGversion(min(as.character(getRversion()), "3.6.2"))
knitr::opts_chunk$set(echo = TRUE)
library(RMaCzek)
library(knitr)
library(tidyr)
library(tidyverse)
library(tinytex)
library(dplyr)
library(readxl)
library(stats)
library(coda)
```

# Q1 - Computations with metropolis-Hastings

*Consider the following probability density function:*

$$f(x)\propto x^5e^{-x}, x > 0$$

*You can see that the distribution is known up to some constant of proportionality. If you are interested (NOT part of the Lab) this constant can be found by applying integration by parts multiple times and equals 120.*

*Use Metropolis{Hastings algorithm to generate samples from this distribution by using proposal distribution as log-normal $LN(X_t, 1)$, take some starting point. Plot the chain you obtained as a time series plot. What can you guess about the convergence of the chain? If there is a burn{in period, what can be the size of this period?*

```{r}
myMH<-function(nstep,X0,props){
  # step 1: initialize chain to X0
  vN<-1:nstep
  vX<-rep(X0,nstep);
  mysample <- c()
  # step 2
  for (i in 2:nstep){
    X<-vX[i-1]
    Y<-as.numeric(log(abs(rnorm(1,mean=X,sd=props))))
    u<-runif(1)
    
    mysample<-rbind(mysample, c(X,Y))
    
    a<-min(c(1,((Y^5)*exp(-1*Y)*as.numeric(log(dnorm(X,mean=Y,sd=props))))/((X^5)*exp(-X)*as.numeric(log(dnorm(Y,mean=X,sd=props))))))
    if (u <=a){
      vX[i]<-Y
    }else{
      vX[i]<-X
    }    
  }
  
  plot(vN,vX,pch=19,cex=0.3,col="black",xlab="t",ylab="X(t)",main="",ylim=c(min(X0-0.5,-5),max(5,X0+0.5)))
  abline(h=0)
  abline(h=1.96)
  abline(h=-1.96)
  
  return(mysample)
}

myMH(1000, 5, 2)
```

## 2.

```{r}
myMH2<-function(nstep,X0,df){
  # step 1: initialize chain to X0
  vN<-1:nstep
  vX<-rep(X0,nstep);
  mysample<-c()
  # step 2:
  for (i in 2:nstep){
    X<-vX[i-1]
    Y <- as.numeric(floor((dchisq(X+1, df = df))))
    u<-runif(1)
    
    mysample<-rbind(mysample, c(X,Y))
    
    a<-min(c(1,((Y^5)*exp(-1*Y)*dchisq(X+1, df = df))/((X^5)*exp(-X)*dchisq(Y, df = df))))
    if (u <= a){
      vX[i]<-Y
    }else{
      vX[i]<-X
    }    
  }
  
  plot(vN,vX,pch=19,cex=0.3,col="black",xlab="t",ylab="X(t)",main="",ylim=c(min(X0-0.5,-5),max(5,X0+0.5)))
  abline(h=0)
  abline(h=1.96)
  abline(h=-1.96)
  
  return(mysample)
}

myMH2(10000, 10, 1)
```

## 3.

*Compare the results of Steps 1 and 2 and make conclusions.*

TEXT

## 4.

*Generate 10 MCMC sequences using the generator from Step 2 and starting points 1:10. Use the Gelman{Rubin method to analyze convergence of these sequences.*

```{r}
for(i in 1:10){
  myMH2(10000, i, 1)
}
```

## 5. 

```{r}
integrand <- function(x){
  x*(x^5)*(exp(-x))
}

MH <- myMH(10000, 5, 2)
int_myLN <- integrate(integrand, lower = min(MH[,1]), upper = max(MH[,1]))

MH2 <- myMH2(10000, 10, 1)
int_myChi2 <- integrate(integrand, lower = min(MH2[,1]), upper = max(MH2[,1]))

int_myLN
int_myChi2
```

## 6.

```{r}
new_integrand <- function(x){
  x*(x^5)*(exp(-x))
}

result <- integrate(integrand, lower = 0, upper = Inf)
result
```

\newpage

# Q2 - Gibbs sampling

*A concentration of a certain chemical was measured in a water sample, and the result was stored in the data chemical.RData having the following variables: X: day of the measurement; and Y: measured concentration of the chemical. The instrument used to measure the concentration had certain accuracy; this is why the measurements can be treated as noisy. Your purpose is to restore the expected concentration values.*

## 1.

*Import the data to R and plot the dependence of Y on X. What kind of model is reasonable to use here?*

```{r, echo=F}
load("chemical.RData")

plot(X, Y)
```

The graph implies a logartihmic relationship. Thus, such a model might work.

## 2. 

*A researcher has decided to use the following (random{walk) Bayesian model (n=number of observations,* 
$$\overrightarrow{\mu} = (\mu_1,...,\mu_n)$$ 

*are unknown parameters):*

$$Y_i \sim N(\mu_i, variance = 0.2), i = 1,...,n$$

*where the prior is*

$$p(|\mu_1) = 1$$
$$p(\mu_{i+1}|\mu_i) = N(\mu,0.2),i, = 1,...,n1$$
*Present the formulae showing the likelihood*
$p(\overrightarrow{Y})$ 
*and the prior*
$p(\mu)$. 
*Hint: a chain rule can be used here*
$p(\overrightarrow\mu) = p(\mu_1)p(\mu_2|\mu_1)p(\mu_3|\mu_2)...p(\mu_n|\mu_n1)$

The the formula for the likelihood of 
$$p({Y_i}|{\mu_i)} = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(Y_1-\mu_1)^2} + \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(Y_2-\mu_2)^2}+...+\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(Y_n-\mu_n)^2}=(\frac{1}{\sqrt{2\pi\sigma^2}})^ne^{-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(Y_i-\mu_i)^2}$$

and the likelihood of

$$p(\overrightarrow{\mu}) = 1 + \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(\mu_2-\mu_1)^2} + \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(\mu_3-\mu_2)^2}+...+\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(\mu_n-\mu_{n-1})^2}=(\frac{1}{\sqrt{2\pi\sigma^2}})^{n-1}e^{-\frac{1}{2\sigma^2}\sum_{i=2}^{n}(\mu_i-\mu_{i-1})^2}$$


## 3.

*Use Bayes' Theorem to get the posterior up to a constant proportionality, and then find out the distributions of $(\mu_i|\overrightarrow{\mu}_{-i},\overrightarrow{Y})$, where $\mu_{-i}$ is a vector containing all $\mu$ values except of $\mu_i$.*

$$ f(\mu|Y)\propto f(\mu)f({Y|\mu}) = (\frac{1}{\sqrt{2\pi\sigma^2}})^{2n-1}e^{-\frac{1}{\sigma^2}\sum_{i=2}^{n}(\mu_i-\mu_{i-1})^2\sum_{i=1}^{n}(Y_i-\mu_{i})^2}$$



```{r}
f.MCMC.Gibbs<-function(nstep,X0,vmean,mVar){
  vN<-1:nstep
  d<-length(vmean)
  mX<-matrix(0,nrow=nstep,ncol=d)
  mX[1,]<-X0
  
  for (i in 2:nstep){
    X<-mX[i-1,]
    Y<-rep(0,d)
    Y[1]<-rnorm(1,mean=vmean[1]+(mVar[1,-1]%*%solve(mVar[-1,-1]))%*%(X[2:d]-vmean[-1]),sd=sqrt(mVar[1,1]-mVar[1,-1]%*%solve(mVar[-1,-1])%*%mVar[-1,1]))
    for (j in 2:(d-1)){
      Y[j]<-rnorm(1,mean=vmean[j]+(mVar[j,-j]%*%solve(mVar[-j,-j]))%*%(c(Y[1:(j-1)],X[(j+1):d])-vmean[-j]),sd=sqrt(mVar[j,j]-mVar[j,-j]%*%solve(mVar[-j,-j])%*%mVar[-j,j]))
    }
    Y[d]<-rnorm(1,mean=vmean[d]+(mVar[d,-d]%*%solve(mVar[-d,-d]))%*%(Y[1:(d-1)]-vmean[-d]),sd=sqrt(mVar[d,d]-mVar[d,-d]%*%solve(mVar[-d,-d])%*%mVar[-d,d]))
    mX[i,]<-Y
  }
  mX
}

vmean<-c(1,2)
mVar<-rbind(c(1,0.5),c(0.5,1))
nstep<-200
X0<-c(10,10)
mX<-f.MCMC.Gibbs(nstep,X0,vmean,mVar)

plot(mX[-1,1],mX[-1,2],pch=19,cex=0.5,col="black",xlab="X1",ylab="X2",main="",cex.lab=1.7,cex.axis=1.5,xlim=c(min(mX[-1,1]-0.5),max(mX[-1,1]+0.5)),ylim=c(min(mX[,2]-0.5),max(mX[-1,2]+0.5)))

par(mfrow=c(2,1))
plot(2:nstep,mX[-1,1],pch=19,cex=0.3,col="black",xlab="t",ylab="X1",cex.axis=1.5,cex.lab=1.7)
abline(h=vmean[1])
plot(2:nstep,mX[-1,2],pch=19,cex=0.3,col="black",xlab="t",ylab="X2",cex.axis=1.5,cex.lab=1.7)
abline(h=vmean[2])


```


\newpage

# Appendix

```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE,results='show'}
```

